{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkbnh/anchor-zero-copy-example/blob/main/demo/vibevoice_realtime_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WvIaUJD2y0yU",
      "metadata": {
        "id": "WvIaUJD2y0yU"
      },
      "source": [
        "# VibeVoice-Realtime Colab — T4 Quickstart\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb0dc915"
      },
      "source": [
        "# Install the requests library if you haven't already\n",
        "!pip install requests --quiet"
      ],
      "id": "bb0dc915",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e773647"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "server_url = \"http://localhost:8000\"\n",
        "\n",
        "# Example: Sending a GET request to a health check endpoint (if available)\n",
        "# Most servers have a /health or /status endpoint\n",
        "try:\n",
        "    response = requests.get(f\"{server_url}/health\")\n",
        "    print(\"Health Check Response:\", response.status_code, response.text)\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(f\"Could not connect to the server at {server_url}. Make sure the VibeVoice server cell is still running.\")\n",
        "\n",
        "# Example: Sending a POST request to an inference endpoint\n",
        "# You would replace 'your_inference_endpoint' with the actual endpoint\n",
        "# and 'your_audio_data_base64' with base64 encoded audio data\n",
        "\n",
        "# This is dummy data for demonstration.\n",
        "# You'd typically load and base64 encode an audio file here.\n",
        "dummy_audio_data = \"SGVsbG8sIHRoaXMgandzdCBhdWRpbyBkYXRhLiI=\" # Base64 encoded 'Hello, this is just audio data.'\n",
        "data_to_send = {\n",
        "    \"audio_base64\": dummy_audio_data,\n",
        "    \"param1\": \"value1\",\n",
        "    \"param2\": \"value2\"\n",
        "}\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "try:\n",
        "    # Replace '/your_inference_endpoint' with the actual endpoint for VibeVoice inference\n",
        "    # For the VibeVoice demo, this might be something like '/synthesize' or '/convert'\n",
        "    # You'll need to consult the VibeVoice API or source code for exact endpoints\n",
        "    inference_response = requests.post(\n",
        "        f\"{server_url}/generate\", # Example endpoint, check VibeVoice demo code\n",
        "        data=json.dumps(data_to_send),\n",
        "        headers=headers\n",
        "    )\n",
        "    print(\"\\nInference Request Response:\", inference_response.status_code, inference_response.text)\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(f\"Could not connect to the server at {server_url}. Make sure the VibeVoice server cell is still running.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "id": "4e773647",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed9f4e64"
      },
      "source": [
        "Please note that the specific endpoints (e.g., `/health`, `/generate`) and the expected format of the `data_to_send` dictionary will depend on the VibeVoice server's API. You might need to inspect the `vibevoice_realtime_demo.py` script or its documentation for precise details on how to send audio for processing."
      ],
      "id": "ed9f4e64"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "722c1256"
      },
      "source": [
        "### Run VibeVoice Demo Locally (Offline Mode)"
      ],
      "id": "722c1256"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e683db9",
        "outputId": "52089aad-7541-4f49-e875-5ac3af9a394a"
      },
      "source": [
        "import subprocess, threading\n",
        "\n",
        "srv_offline = subprocess.Popen(\n",
        "    \"python /content/VibeVoice/demo/vibevoice_realtime_demo.py --model_path /content/models/VibeVoice-Realtime-0.5B --port 8000\",\n",
        "    shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True,\n",
        ")\n",
        "\n",
        "def read_srv_offline():\n",
        "    for ln in srv_offline.stdout:\n",
        "        print(ln.strip())\n",
        "        if \"Uvicorn running on\" in ln:\n",
        "            print(\"✅ VibeVoice server running locally on port 8000\")\n",
        "\n",
        "threading.Thread(target=read_srv_offline, daemon=True).start()\n",
        "\n",
        "# Keep the script alive to allow the server to run\n",
        "# In a real scenario, you might want to integrate this with other local processes\n",
        "# For now, we'll just indicate it's running.\n",
        "print(\"VibeVoice server started. It is running locally and not exposed to the internet.\")\n",
        "print(\"You can manually stop this cell when you are done.\")"
      ],
      "id": "9e683db9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VibeVoice server started. It is running locally and not exposed to the internet.\n",
            "You can manually stop this cell when you are done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8fTKYGx7DZk",
      "metadata": {
        "id": "e8fTKYGx7DZk"
      },
      "source": [
        "## Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4wxJ6QHM-ZOb",
      "metadata": {
        "id": "4wxJ6QHM-ZOb"
      },
      "outputs": [],
      "source": [
        "# Check for T4 GPU\n",
        "import torch\n",
        "if torch.cuda.is_available() and \"T4\" in torch.cuda.get_device_name(0):\n",
        "    print(\"✅ T4 GPU detected\")\n",
        "else:\n",
        "    print(\"\"\"\n",
        "    ⚠️ WARNING: T4 GPU not detected\n",
        "\n",
        "    The recommended runtime for this Colab notebook is \"T4 GPU\".\n",
        "\n",
        "    To change the runtime type:\n",
        "\n",
        "        1. Click on \"Runtime\" in the top navigation menu\n",
        "        2. Click on \"Change runtime type\"\n",
        "        3. Select \"T4 GPU\"\n",
        "        4. Click \"OK\" if a \"Disconnect and delete runtime\" window appears\n",
        "        5. Click on \"Save\"\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "# Clone the VibeVoice repository\n",
        "![ -d /content/VibeVoice ] || git clone --quiet --branch main --depth 1 https://github.com/microsoft/VibeVoice.git /content/VibeVoice\n",
        "print(\"✅ Cloned VibeVoice repository\")\n",
        "\n",
        "# Install project dependencies\n",
        "!uv pip --quiet install --system -e /content/VibeVoice\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared && chmod +x cloudflared\n",
        "print(\"✅ Installed dependencies\")\n",
        "\n",
        "# Download model\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\"microsoft/VibeVoice-Realtime-0.5B\", local_dir=\"/content/models/VibeVoice-Realtime-0.5B\")\n",
        "print(\"✅ Downloaded model: microsoft/VibeVoice-Realtime-0.5B\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c727ab",
      "metadata": {
        "id": "88c727ab"
      },
      "source": [
        "[Optional] If the download exceeds 1 minute, it is probably stuck. You can: (1) interrupt the execution, (2) log in to Hugging Face, and (3) try download again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec6b870",
      "metadata": {
        "id": "dec6b870"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c579654b",
      "metadata": {
        "id": "c579654b"
      },
      "outputs": [],
      "source": [
        "snapshot_download(\"microsoft/VibeVoice-Realtime-0.5B\", local_dir=\"/content/models/VibeVoice-Realtime-0.5B\")\n",
        "print(\"✅ Downloaded model: microsoft/VibeVoice-Realtime-0.5B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pgKlV7153Ifi",
      "metadata": {
        "id": "pgKlV7153Ifi"
      },
      "source": [
        "## Step 2: Launch VibeVoice-Realtime Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yc1N9EHswFxA",
      "metadata": {
        "id": "Yc1N9EHswFxA"
      },
      "outputs": [],
      "source": [
        "import subprocess, re, time, threading\n",
        "\n",
        "srv = subprocess.Popen(\n",
        "    \"python /content/VibeVoice/demo/vibevoice_realtime_demo.py --model_path /content/models/VibeVoice-Realtime-0.5B --port 8000\",\n",
        "    shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True,\n",
        ")\n",
        "cf = subprocess.Popen(\n",
        "    \"./cloudflared tunnel --url http://localhost:8000 --no-autoupdate\",\n",
        "    shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True,\n",
        ")\n",
        "\n",
        "public_url = None\n",
        "server_ready = False\n",
        "url_pattern  = re.compile(r\"(https://[a-z0-9-]+\\.trycloudflare\\.com)\")\n",
        "\n",
        "def read_srv():\n",
        "    global server_ready\n",
        "    for ln in srv.stdout:\n",
        "        print(ln.strip())\n",
        "        if \"Uvicorn running on\" in ln:\n",
        "            server_ready = True\n",
        "\n",
        "def read_cf():\n",
        "    global public_url\n",
        "    for ln in cf.stdout:\n",
        "        m = url_pattern.search(ln)\n",
        "        if m:\n",
        "            public_url = m.group(1)\n",
        "            break\n",
        "\n",
        "threading.Thread(target=read_srv, daemon=True).start()\n",
        "threading.Thread(target=read_cf,  daemon=True).start()\n",
        "\n",
        "\n",
        "while True:\n",
        "    if server_ready and public_url:\n",
        "        print(f\"✅ Public URL: {public_url}\\n\");\n",
        "        public_url = None\n",
        "    time.sleep(0.25)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "name": "VibeVoice_Colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}